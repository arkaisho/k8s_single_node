{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a apache-spark clustre using pykube-ng\n",
    "\n",
    "The purpose of this notebook is to introduce the use of Apache Spark in a kubernetes single node cluster, deploying an Apache Spark master and workers.\n",
    "\n",
    "> Due to connection problems, this Issue was divided into 2 notebooks that have to be run on their specific pods.\n",
    "\n",
    "> This notebook must be run using the jupyter notebook server inside the pod generated by the issue5 \"notebook\" deployment\n",
    "\n",
    "> To these notebooks to work, your kubernetes needs to have some specifications:\n",
    "- the ports 8889, 4040, 4041, 7077, 8080, 7078 must be free\n",
    "\n",
    "##  1 Creating master and workers\n",
    "\n",
    "Similar to previous issues, we will use pykube-ng and .yaml manifests to create the necessary deployments. The .yaml files are inside the folder [\"issue8\"](http://localhost:8888/tree/issue8) located in the home directory of this jupyter server.\n",
    "\n",
    "Run the cells below to create the deployments and services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pykube\n",
    "import yaml\n",
    "from issue8.kube import *\n",
    "\n",
    "api = pykube.HTTPClient(pykube.KubeConfig.from_file(\"k3s.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load master files\n",
    "master_dep_file = open(\"issue8/spark-master-deployment.yaml\", \"r\")\n",
    "master_svc_file = open(\"issue8/spark-master-service.yaml\", \"r\")\n",
    "#load workers files\n",
    "worker_dep_file = open(\"issue8/spark-worker-deployment.yaml\", \"r\")\n",
    "worker_svc_file = open(\"issue8/spark-worker-service.yaml\", \"r\")\n",
    "#load entrypoint jupyter files\n",
    "notebook_pod_file = open(\"issue8/pyspark-notebook-pod.yaml\", \"r\")\n",
    "notebook_svc_file = open(\"issue8/pyspark-notebook-service.yaml\", \"r\")\n",
    "\n",
    "#create a list for the manifests\n",
    "master_specs = []\n",
    "worker_specs = []\n",
    "notebook_specs = []\n",
    "\n",
    "master_specs.append(yaml.load(master_dep_file.read(), Loader=yaml.FullLoader))\n",
    "master_specs.append(yaml.load(master_svc_file.read(), Loader=yaml.FullLoader))\n",
    "worker_specs.append(yaml.load(worker_dep_file.read(), Loader=yaml.FullLoader))\n",
    "worker_specs.append(yaml.load(worker_svc_file.read(), Loader=yaml.FullLoader))\n",
    "notebook_specs.append(yaml.load(notebook_pod_file.read(), Loader=yaml.FullLoader))\n",
    "notebook_specs.append(yaml.load(notebook_svc_file.read(), Loader=yaml.FullLoader))\n",
    "\n",
    "#close all files\n",
    "master_dep_file.close()\n",
    "master_svc_file.close()\n",
    "worker_dep_file.close()\n",
    "worker_svc_file.close()\n",
    "notebook_pod_file.close()\n",
    "notebook_svc_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> These files create three pods and make some alterations: \n",
    "- A [bde2020/spark-master](https://hub.docker.com/r/bde2020/spark-master/) pod;\n",
    "  - Expose the pod at ports 8080 and 7077 (WebUi and Master ports respectively)\n",
    "  - Create the PYSPARK_PYTHON environment variable to define python3 as default spark python\n",
    "- A [bde2020/spark-worker](https://hub.docker.com/r/bde2020/spark-worker/) pod;\n",
    "  - Expose the pod at port 7078\n",
    "  - Create the PYSPARK_PYTHON environment variable to define python3 as default spark python\n",
    "  - Create the SPARK_MASTER environment variable to define the spark master adress to connection\n",
    "  - Create the MASTER_HOST environment variable to define the spark master adress inside kuberetes\n",
    "- A [jupyter/pyspark-notebook:ubuntu-18.04](https://hub.docker.com/r/jupyter/pyspark-notebook/) pod;\n",
    "  - Expose the pod at ports 8889, 4040, 4041 (Jupyter, WebUi and Communication respectively)\n",
    "  - Configure the pod to start the notebook with no authentication and at the 8889 port\n",
    "  - Create the PYSPARK_PYTHON environment variable t\"/home/jovyan/work\"o define python3 as default spark python\n",
    "  - Configure the pod workdir to \"/home/jovyan/work\"\n",
    "  - Mount the same volume used in this notebook inside the pyspark-notebook pod workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cluster is starting ...\n",
      "The cluster is ready\n"
     ]
    }
   ],
   "source": [
    "#constroy the manifests objects\n",
    "for spec in master_specs:\n",
    "    constroy(api,spec)\n",
    "for spec in worker_specs:\n",
    "    constroy(api,spec)\n",
    "for spec in notebook_specs:\n",
    "    constroy(api,spec)\n",
    "    \n",
    "#wait until the cluster get ready\n",
    "import time\n",
    "starting = True\n",
    "print(\"The cluster is starting ...\")\n",
    "while(starting):\n",
    "    time.sleep(1)\n",
    "    pods = pykube.Pod.objects(api)\n",
    "    for pod in pods:\n",
    "        starting = not pod.ready\n",
    "        if starting:\n",
    "            break\n",
    "print(\"The cluster is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Testing the cluster\n",
    "\n",
    "Once our cluster is ready to use, go to the [entrypoint notebook](http://localhost:8889/notebooks/Issue8(pyspark-test).ipynb) and run a simple task to test the cluster.\n",
    "\n",
    "> Run the [entrypoint notebook](http://localhost:8889/notebooks/Issue8(pyspark-test).ipynb) code twice, before and after scaling the cluster to 2 workers and see the diference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default spark-worker replicas 2\n",
      "The cluster is starting ...\n",
      "The cluster is ready\n"
     ]
    }
   ],
   "source": [
    "scale_cluster(api,\"spark-worker\",2,\"default\")\n",
    "starting = True\n",
    "print(\"The cluster is scaling ...\")\n",
    "while(starting):\n",
    "    time.sleep(1)\n",
    "    pods = pykube.Pod.objects(api)\n",
    "    for pod in pods:\n",
    "        starting = not pod.ready\n",
    "        if starting:\n",
    "            break\n",
    "print(\"The cluster is ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Destroy the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for spec in worker_specs:\n",
    "    destroy(api,spec)\n",
    "for spec in master_specs:\n",
    "    destroy(api,spec)\n",
    "for spec in notebook_specs:\n",
    "    destroy(api,spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
